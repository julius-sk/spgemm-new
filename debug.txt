def validate_against_cusparse(self, graph_data, input_features, dim_k, tolerance=0.001, use_cuda_topk=True):
    # ... existing code ...
    
    # Step 4: Both should have same shape - compare directly
    if maxk_output.shape != cusparse_output.shape:
        print(f"‚ùå Shape mismatch! MaxK: {maxk_output.shape}, cuSPARSE: {cusparse_output.shape}")
        return False
    
    # Step 5: Compare values at the positions where input was non-zero
    input_nonzero_mask = sparse_input != 0
    diff = torch.abs(maxk_output - cusparse_output)
    
    # Find the exact location of the maximum error
    if input_nonzero_mask.any():
        # Get the max error value
        max_error_value = diff.max().item()
        
        # Find the location of the max error
        max_error_indices = torch.where(diff == max_error_value)
        row_idx = max_error_indices[0][0].item()
        col_idx = max_error_indices[1][0].item()
        
        # Get values at this position from both outputs
        maxk_value = maxk_output[row_idx, col_idx].item()
        cusparse_value = cusparse_output[row_idx, col_idx].item()
        
        # Check if the column is in the non-zero set for this row
        # Get the indices where this row had non-zero values in input
        row_nonzero_cols = topk_indices_int64[row_idx]
        is_col_in_topk = col_idx in row_nonzero_cols
        
        print(f"\nüìç MAX ERROR LOCATION:")
        print(f"   Row: {row_idx}, Column: {col_idx}")
        print(f"   MaxK value: {maxk_value:.8f}")
        print(f"   cuSPARSE value: {cusparse_value:.8f}")
        print(f"   Difference: {max_error_value:.8f}")
        print(f"   Column is in row's TopK: {is_col_in_topk}")
        
        # More debug info - print nearby values
        print(f"\nüìä CONTEXT AROUND ERROR:")
        
        # Get TopK indices for this row
        topk_cols = topk_indices_int64[row_idx].tolist()
        print(f"   TopK columns for row {row_idx}: {topk_cols[:5]}...")
        
        # Print the input values at TopK positions
        print(f"   Input values at TopK for row {row_idx}:")
        for i, col in enumerate(topk_cols[:5]):
            print(f"     Col {col}: {sparse_input[row_idx, col].item():.6f}")
        
        # Print outputs around error position
        print(f"   MaxK outputs around error position:")
        start_col = max(0, col_idx - 2)
        end_col = min(maxk_output.shape[1] - 1, col_idx + 2)
        for c in range(start_col, end_col + 1):
            v = maxk_output[row_idx, c].item()
            highlight = " <-- ERROR" if c == col_idx else ""
            print(f"     Col {c}: {v:.6f}{highlight}")
        
        # Check pattern of errors in the row
        row_diffs = diff[row_idx]
        large_errors = torch.where(row_diffs > 0.1)[0]
        if len(large_errors) > 0:
            print(f"   Number of large errors in row {row_idx}: {len(large_errors)}")
            print(f"   Large error columns: {large_errors[:10].tolist()}")
    
    relevant_diff = diff[input_nonzero_mask]
    max_error = relevant_diff.max().item() if relevant_diff.numel() > 0 else 0.0
    avg_error = relevant_diff.mean().item() if relevant_diff.numel() > 0 else 0.0
    
    print(f"\nüìä OVERALL ERROR STATISTICS:")
    print(f"   Max error: {max_error:.8f}")
    print(f"   Avg error: {avg_error:.8f}")
    print(f"   Tolerance: {tolerance}")
    
    # Count errors above different thresholds
    error_count_01 = (diff > 0.1).sum().item()
    error_count_05 = (diff > 0.5).sum().item()
    error_count_09 = (diff > 0.9).sum().item()
    total_elements = diff.numel()
    
    print(f"   Elements with error > 0.1: {error_count_01} ({error_count_01/total_elements*100:.6f}%)")
    print(f"   Elements with error > 0.5: {error_count_05} ({error_count_05/total_elements*100:.6f}%)")
    print(f"   Elements with error > 0.9: {error_count_09} ({error_count_09/total_elements*100:.6f}%)")
    
    is_valid = max_error < tolerance
    
    # ... rest of the function ...
